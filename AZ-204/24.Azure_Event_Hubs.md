# Azure Event Hubs Key Points

- **Event Hubs**: Front door for an event pipeline, also called *event ingestor* ; decouples event stream production from consumption.

## Key Features

| Feature                    | Description                                                                                                                |
|----------------------------|----------------------------------------------------------------------------------------------------------------------------|
| Fully managed PaaS         | Event Hubs is a fully managed PaaS with little configuration/management overhead. Event Hubs for Apache Kafka ecosystems.  |
| Real-time and batch processing | Partitioned consumer model; multiple applications can process the stream concurrently.                                   |
| Capture event data         | Near-real-time data capture in Azure Blob storage or Azure Data Lake Storage.                                               |
| Scalable                   | Auto-inflate scaling options to meet usage needs.                                                                           |
| Rich ecosystem             | Supports Apache Kafka clients (1.0 and later).                                                                              |

## Key Components

- **Event Hubs client**: Primary interface for interacting with the Event Hubs client library.
- **Event Hubs producer**: Client that sends telemetry data, diagnostics, usage logs, etc.
- **Event Hubs consumer**: Client that reads and processes event information; supports aggregation, computation, filtering, distribution, or storage.
- **Partition**: Ordered sequence of events held in Event Hubs; supports parallelism for event consumers.
- **Consumer group**: View of an entire Event Hubs; allows multiple consuming applications to read streams independently.
- **Event receivers**: Entities that read event data from Event Hubs; connect via AMQP 1.0 or Kafka protocol 1.0 and later.
- **Throughput units/processing units**: Prepurchased units controlling the throughput capacity of Event Hubs.

- ## Diagram of Azure Event Hubs Stream Processing Architecture

<img src="https://learn.microsoft.com/en-in/training/wwl-azure/azure-event-hubs/media/event-hubs-stream-processing.png" alt="Azure Event Hubs Stream Processing Architecture" width="500"/>

**Description of the Processes in the Diagram**:
- **Event Producers**: Generate events and send them to Event Hubs using HTTPS, AMQP, or Kafka protocols.
- **Azure Event Hubs**: Receives events and stores them in partitions.
- **Partitions**: Organize events in a sequence; each partition is read by specific consumers.
- **Consumer Groups**: Enable multiple consuming applications to read the event stream independently.
- **Event Receivers**: Consume events from partitions within their consumer group.

## Explore Event Hubs Capture

Azure Event Hubs allows automatic capture of streaming data in Event Hubs into Azure Blob storage or Azure Data Lake Storage, with options to specify time or size intervals. It is cost-effective, easy to set up, and scales with Event Hubs throughput units.

![Event Hubs Capture](https://learn.microsoft.com/en-in/training/wwl-azure/azure-event-hubs/media/event-hubs-capture.png)

### Key Features
- Automatic data capture from Event Hubs
- Storage in Azure Blob or Data Lake Storage
- Flexible interval settings (time or size)
- No administrative costs
- Automatic scaling with throughput units

### Benefits
- Process real-time and batch-based pipelines
- Build scalable solutions

Event Hubs Capture helps manage and process data efficiently, supporting various scaling needs over time.

## How Event Hubs Capture Works

Event Hubs acts as a durable buffer for telemetry ingress, functioning like a distributed log with time-retention. It scales using a partitioned consumer model, where each partition is an independent segment of data, consumed separately. Data ages off based on a configurable retention period, preventing the event hub from getting "too full."

### Key Features

- **Partitioned Consumer Model**: Each partition is an independent segment, consumed independently.
- **Configurable Retention Period**: Data ages off based on this period, ensuring the hub doesn't overflow.
- **Flexibility in Storage**: Choose your Azure Blob storage account and container, or Azure Data Lake Store account. Storage can be in the same or different region as the event hub.

### Captured Data Format

- **Apache Avro Format**: Captured data is written in Avro format, known for its compact, fast, binary structure, and rich data schemas.
- **Usage**: Widely used in Hadoop ecosystem, Stream Analytics, and Azure Data Factory.

### Benefits

- **Scalability**: Efficient scaling with independent partition consumption.
- **Storage Flexibility**: Ability to store data in various regions.
- **Efficient Data Handling**: Avro format ensures compact and fast data processing.

## Capture Windowing

Event Hubs Capture allows you to configure a capture window with minimum size and time settings. The "first wins policy" applies, where the first trigger (either size or time) initiates the capture operation.

### Key Features

- **Independent Partition Capture**: Each partition captures data independently.
- **Trigger Mechanism**: Capture is triggered by either size or time, whichever is encountered first.
- **Naming**: Completed block blobs are named based on the time of capture interval.

### Benefits

- **Control Over Capturing**: Customize capture windows to suit your needs.
- **Efficient Data Management**: Independent captures for each partition ensure streamlined data handling.
- **Clear Organization**: Blobs are named according to capture times, facilitating easy data retrieval.


The storage naming convention is as follows:
```
{Namespace}/{EventHub}/{PartitionId}/{Year}/{Month}/{Day}/{Hour}/{Minute}/{Second}
```
Note the date values are padded with zeroes; an example filename might be:
```
https://mystorageaccount.blob.core.windows.net/mycontainer/mynamespace/myeventhub/0/2017/12/08/03/03/17.avro
```

### Scaling to Throughput Units

- **Throughput Units (TUs):**
  - Each TU allows:
    - 1 MB/s or 1000 events/s ingress
    - 2 MB/s or 2000 events/s egress
  - Standard Event Hubs: 1-20 TUs configurable
  - More TUs available via quota increase support request
  - Usage beyond purchased TUs is throttled

- **Event Hubs Capture:**
  - Copies data directly from internal storage
  - Bypasses TU egress quotas
  - Saves egress for other processing readers (e.g., Stream Analytics, Spark)
  - Runs automatically upon first event
  - Writes empty files when no data to ensure predictable cadence for batch processors
