# Azure Event Hubs Key Points

- **Event Hubs**: Front door for an event pipeline, also called *event ingestor* ; decouples event stream production from consumption.

## Key Features

| Feature                    | Description                                                                                                                |
|----------------------------|----------------------------------------------------------------------------------------------------------------------------|
| Fully managed PaaS         | Event Hubs is a fully managed PaaS with little configuration/management overhead. Event Hubs for Apache Kafka ecosystems.  |
| Real-time and batch processing | Partitioned consumer model; multiple applications can process the stream concurrently.                                   |
| Capture event data         | Near-real-time data capture in Azure Blob storage or Azure Data Lake Storage.                                               |
| Scalable                   | Auto-inflate scaling options to meet usage needs.                                                                           |
| Rich ecosystem             | Supports Apache Kafka clients (1.0 and later).                                                                              |

## Key Components

- **Event Hubs client**: Primary interface for interacting with the Event Hubs client library.
- **Event Hubs producer**: Client that sends telemetry data, diagnostics, usage logs, etc.
- **Event Hubs consumer**: Client that reads and processes event information; supports aggregation, computation, filtering, distribution, or storage.
- **Partition**: Ordered sequence of events held in Event Hubs; supports parallelism for event consumers.
- **Consumer group**: View of an entire Event Hubs; allows multiple consuming applications to read streams independently.
- **Event receivers**: Entities that read event data from Event Hubs; connect via AMQP 1.0 or Kafka protocol 1.0 and later.
- **Throughput units/processing units**: Prepurchased units controlling the throughput capacity of Event Hubs.

- ## Diagram of Azure Event Hubs Stream Processing Architecture

<img src="https://learn.microsoft.com/en-in/training/wwl-azure/azure-event-hubs/media/event-hubs-stream-processing.png" alt="Azure Event Hubs Stream Processing Architecture" width="500"/>

**Description of the Processes in the Diagram**:
- **Event Producers**: Generate events and send them to Event Hubs using HTTPS, AMQP, or Kafka protocols.
- **Azure Event Hubs**: Receives events and stores them in partitions.
- **Partitions**: Organize events in a sequence; each partition is read by specific consumers.
- **Consumer Groups**: Enable multiple consuming applications to read the event stream independently.
- **Event Receivers**: Consume events from partitions within their consumer group.

## Explore Event Hubs Capture

Azure Event Hubs allows automatic capture of streaming data in Event Hubs into Azure Blob storage or Azure Data Lake Storage, with options to specify time or size intervals. It is cost-effective, easy to set up, and scales with Event Hubs throughput units.

![Event Hubs Capture](https://learn.microsoft.com/en-in/training/wwl-azure/azure-event-hubs/media/event-hubs-capture.png)

### Key Features
- Automatic data capture from Event Hubs
- Storage in Azure Blob or Data Lake Storage
- Flexible interval settings (time or size)
- No administrative costs
- Automatic scaling with throughput units

### Benefits
- Process real-time and batch-based pipelines
- Build scalable solutions

Event Hubs Capture helps manage and process data efficiently, supporting various scaling needs over time.

## How Event Hubs Capture Works

Event Hubs acts as a durable buffer for telemetry ingress, functioning like a distributed log with time-retention. It scales using a partitioned consumer model, where each partition is an independent segment of data, consumed separately. Data ages off based on a configurable retention period, preventing the event hub from getting "too full."

### Key Features

- **Partitioned Consumer Model**: Each partition is an independent segment, consumed independently.
- **Configurable Retention Period**: Data ages off based on this period, ensuring the hub doesn't overflow.
- **Flexibility in Storage**: Choose your Azure Blob storage account and container, or Azure Data Lake Store account. Storage can be in the same or different region as the event hub.

### Captured Data Format

- **Apache Avro Format**: Captured data is written in Avro format, known for its compact, fast, binary structure, and rich data schemas.
- **Usage**: Widely used in Hadoop ecosystem, Stream Analytics, and Azure Data Factory.

### Benefits

- **Scalability**: Efficient scaling with independent partition consumption.
- **Storage Flexibility**: Ability to store data in various regions.
- **Efficient Data Handling**: Avro format ensures compact and fast data processing.

## Capture Windowing

Event Hubs Capture allows you to configure a capture window with minimum size and time settings. The "first wins policy" applies, where the first trigger (either size or time) initiates the capture operation.

### Key Features

- **Independent Partition Capture**: Each partition captures data independently.
- **Trigger Mechanism**: Capture is triggered by either size or time, whichever is encountered first.
- **Naming**: Completed block blobs are named based on the time of capture interval.

### Benefits

- **Control Over Capturing**: Customize capture windows to suit your needs.
- **Efficient Data Management**: Independent captures for each partition ensure streamlined data handling.
- **Clear Organization**: Blobs are named according to capture times, facilitating easy data retrieval.


The storage naming convention is as follows:
```
{Namespace}/{EventHub}/{PartitionId}/{Year}/{Month}/{Day}/{Hour}/{Minute}/{Second}
```
Note the date values are padded with zeroes; an example filename might be:
```
https://mystorageaccount.blob.core.windows.net/mycontainer/mynamespace/myeventhub/0/2017/12/08/03/03/17.avro
```

### Scaling to Throughput Units

- **Throughput Units (TUs):**
  - Each TU allows:
    - 1 MB/s or 1000 events/s ingress
    - 2 MB/s or 2000 events/s egress
  - Standard Event Hubs: 1-20 TUs configurable
  - More TUs available via quota increase support request
  - Usage beyond purchased TUs is throttled

- **Event Hubs Capture:**
  - Copies data directly from internal storage
  - Bypasses TU egress quotas
  - Saves egress for other processing readers (e.g., Stream Analytics, Spark)
  - Runs automatically upon first event
  - Writes empty files when no data to ensure predictable cadence for batch processors


### Scale Your Processing Application

- **Multiple Instances:**
  - Run multiple instances of the application for load balancing.
  - Older versions: Use EventProcessorHost for load balancing and checkpointing.
  - Newer versions (5.0+): Use EventProcessorClient (.NET/Java) or EventHubConsumerClient (Python/JavaScript).

- **Partitioned Consumers:**
  - Enables high scale by removing contention bottleneck.
  - Facilitates end-to-end parallelism.

### Example Scenario: Home Security Monitoring

- **Setup:**
  - 100,000 homes with various sensors.
  - Data pushed to an event hub with 16 partitions.
  - Mechanism needed to read, consolidate, and store events for user-friendly web display.

- **Design Requirements:**
  - **Scale:** Multiple consumers, each owning a few partitions.
  - **Load Balance:** Dynamically adjust consumers based on event volume.
  - **Seamless Resume:** Handle failures by other consumers picking up from the last checkpoint.
  - **Consume Events:** Code to aggregate and upload events to blob storage.

### Event Processor or Consumer Client

- **SDKs:** 
  - .NET/Java: Use EventProcessorClient.
  - Python/JavaScript: Use EventHubConsumerClient.
  - Recommended for production scenarios to manage distribution and balancing of work.

### Partition Ownership Tracking

- **Ownership:** 
  - Each processor instance owns and processes one or more partitions.
  - Ownership managed through a checkpoint store.
  - Load balanced among active processors.

### Receive Messages

- **Processing Functions:**
  - Specify functions for event and error processing.
  - Handle each event from a specific partition.
  - Implement retry logic to ensure at least once message processing.

### Checkpointing

- **Process:**
  - Marks the position of the last successfully processed event.
  - Ensures resiliency and allows for recovery by starting from the last checkpoint.
  - Can revert to older data by specifying a lower offset.

### Thread Safety and Processor Instances

- **Event Processing:**
  - Events from the same partition processed sequentially.
  - Events from different partitions can be processed concurrently.
  - Synchronize shared state across partitions.

### Control Access to Events

#### Authentication and Authorization Methods
- **Microsoft Entra ID**
- **Shared Access Signatures (SAS)**

#### Azure Built-in Roles for Event Hubs
- **Azure Event Hubs Data Owner:** Full access to Event Hubs resources.
- **Azure Event Hubs Data Sender:** Send access to Event Hubs resources.
- **Azure Event Hubs Data Receiver:** Receive access to Event Hubs resources.

#### Authorize Access with Managed Identities
- **Azure Role-Based Access Control (RBAC):**
  - Assign Azure roles to managed identities.
  - Grants access to send and read from Event Hubs.

#### Authorize Access with Microsoft Identity Platform
- **OAuth 2.0 Access Tokens:**
  - Request tokens from Microsoft Entra ID.
  - No need to store credentials in code.
  - Authenticates users, groups, or service principals.
  - Returns access tokens for authorizing Event Hubs requests.

#### Authorize Access to Event Hubs Publishers with SAS
- **Virtual Endpoint for Event Hubs:**
  - Publishers can only send messages, not receive.
  - Fine-grained access control per client.
  - Unique token per client, valid for one publisher.
  - Shared access signature keys prevent token manufacturing.

#### Authorize Access to Event Hubs Consumers with SAS
- **Back-End Application Authentication:**
  - Clients need manage rights or listen privileges.
  - Data consumed using consumer groups.
  - SAS policies provide granular scope at entity level.
  - Privileges apply to consumer groups at namespace or event hub instance level.

### Common Operations with the Event Hubs Client Library

#### Inspect Event Hubs
- **Query Partitions:**
  - Many operations occur within specific partitions.
  - Partition names are assigned at creation.
  - Use `EventHubProducerClient` to query available partitions.

#### Publish Events to Event Hubs
- **Create EventHubProducerClient:**
  - Publishes events in batches.
  - Can specify a partition or use automatic routing.
  - **Recommendation:** Use automatic routing for high availability and even distribution.

#### Read Events from an Event Hubs
- **Create EventHubConsumerClient:**
  - Use for a given consumer group.
  - Default consumer group available upon Event Hubs creation.
  - **Note:** This approach is for exploration and prototyping. For production, use `EventProcessorClient`.

#### Read Events from an Event Hubs Partition
- **Specify Partition:**
  - Define the starting point in the event stream.
  - Example focuses on reading all events from the first partition.

#### Process Events Using Event Processor Client
- **Recommended for Production:**
  - Use `EventProcessorClient` for robust and performant event processing.
  - Requires `BlobContainerClient` for Azure Storage blob persistence.
  - Configure the storage account and container for the processor.


